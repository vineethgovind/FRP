{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ca439bd7-111e-4d1c-b895-914d0521c6aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error: 102.1100847855011\n",
      "Root Mean Squared Error: 10.104953477651497\n",
      "R-squared Error: 0.9833423839415076\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor,StackingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split, TimeSeriesSplit\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import boto3\n",
    "from botocore.exceptions import ClientError\n",
    "import time\n",
    "\n",
    "\n",
    "#for streaming data uncomment the following code \n",
    "  \"\"\" def __init__(self, data_file):\n",
    "        self.data_file = data_file\n",
    "        self.chunk_size = 1000\n",
    "        self.df = pd.DataFrame()\n",
    "\n",
    "    def preprocess_data(self, chunk):\n",
    "        # Drop unnecessary columns\n",
    "        chunk = chunk.drop(['track', 'instrument', 'version'], axis=1)\n",
    "\n",
    "        # Map categorical variables to numeric values\n",
    "        chunk['satellite'] = chunk['satellite'].map({'Terra': 0, 'Aqua': 1})\n",
    "        chunk['daynight'] = chunk['daynight'].map({'D': 0, 'N': 1})\n",
    "\n",
    "        # Create month column\n",
    "        chunk['month'] = pd.to_datetime(chunk['acq_date']).dt.month\n",
    "\n",
    "        # Drop rows with missing values\n",
    "        chunk.dropna(inplace=True)\n",
    "\n",
    "        return chunk\"\"\"\n",
    "\n",
    "class Modis:\n",
    "    \n",
    "    def __init__(self, data_file):\n",
    "        self.df = pd.read_csv(data_file)\n",
    "        #self.df = self.preprocess_data()\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        # Drop unnecessary columns\n",
    "        self.df = self.df.drop(['track', 'instrument', 'version'], axis=1)\n",
    "\n",
    "        # Map categorical variables to numeric values\n",
    "        self.df['satellite'] = self.df['satellite'].map({'Terra': 0, 'Aqua': 1})\n",
    "        self.df['daynight'] = self.df['daynight'].map({'D': 0, 'N': 1})\n",
    "\n",
    "        # Create month column\n",
    "        self.df['month'] = pd.to_datetime(self.df['acq_date']).dt.month\n",
    "\n",
    "        # Sample data to reduce computation time\n",
    "        self.df = self.df.sample(frac=0.2, random_state=42)\n",
    "        self.df = self.df.reset_index(drop=True)\n",
    "\n",
    "        # Drop rows with missing values\n",
    "        self.df.dropna(inplace=True)\n",
    "\n",
    "        return self.df\n",
    "\n",
    "    def train_test_split(self):\n",
    "        # Split data into train and test sets\n",
    "        X = self.df[['latitude', 'longitude', 'month', 'brightness', 'scan', 'acq_time', 'bright_t31', 'daynight']]\n",
    "        y = self.df['frp']\n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "        # Scale the data\n",
    "        scaler = StandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        return X_train_scaled, X_test_scaled, y_train, y_test\n",
    "\n",
    "    def model_training(self, X_train_scaled, y_train):\n",
    "        # Train the models\n",
    "        estimators = [\n",
    "            ('rf', RandomForestRegressor(n_estimators=100, random_state=42)),\n",
    "            ('gb', GradientBoostingRegressor(n_estimators=100, random_state=42)),\n",
    "            ('xgb', XGBRegressor(n_estimators=100, random_state=42))\n",
    "        ]\n",
    "\n",
    "        stacking_regressor = StackingRegressor(\n",
    "            estimators=estimators,\n",
    "            final_estimator=LinearRegression()\n",
    "        )\n",
    "\n",
    "        stacking_regressor.fit(X_train_scaled, y_train)\n",
    "\n",
    "        return stacking_regressor\n",
    "    \n",
    "    def ensemble_predictions(self, stacking_regressor, X_test_scaled):\n",
    "        # Make ensemble predictions\n",
    "        ensemble_predictions = stacking_regressor.predict(X_test_scaled)       \n",
    "\n",
    "        return ensemble_predictions\n",
    "    \n",
    "\n",
    "    def evaluation(self, y_test, ensemble_predictions):\n",
    "    # Evaluate the model\n",
    "        mse = mean_squared_error(y_test, ensemble_predictions)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r_squared = r2_score(y_test, ensemble_predictions)\n",
    "        print(\"Mean Squared Error:\", mse)\n",
    "        print(\"Root Mean Squared Error:\", rmse)\n",
    "        print(\"R-squared Error:\", r_squared)\n",
    "        \n",
    "\n",
    "    \n",
    "    def check_for_new_data_locally(self, time_limit_hours=24):\n",
    "        #check for new data locally every hour \n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < time_limit_hours * 3600:\n",
    "            # Check for new data here\n",
    "            new_data = pd.read_csv('new_data.csv')\n",
    "            if not new_data.empty:\n",
    "                self.df = pd.concat([self.df, new_data], axis=0)\n",
    "                self.df = self.preprocess_data()\n",
    "\n",
    "            time.sleep(3600) # Check every hour\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "    def check_for_new_data_from_s3(self, time_limit_hours=24):\n",
    "         #check for new data from a s3 bucket every hour \n",
    "        start_time = time.time()\n",
    "        while time.time() - start_time < time_limit_hours * 3600:\n",
    "            # Check for new data here\n",
    "            try:\n",
    "                s3 = boto3.client('s3')\n",
    "                obj = s3.get_object(Bucket='bucket-name', Key='new_data.csv')\n",
    "                new_data = pd.read_csv(obj['Body'])\n",
    "                if not new_data.empty:\n",
    "                    self.df = pd.concat([self.df, new_data], axis=0)\n",
    "                    self.df = self.preprocess_data()\n",
    "            except ClientError as e:\n",
    "                if e.response['Error']['Code'] == 'NoSuchKey':\n",
    "                    print('The object does not exist.')\n",
    "                else:\n",
    "                    print('An error occurred while trying to read data from the S3 bucket.')\n",
    "\n",
    "            time.sleep(3600) # Check every hour\n",
    "            \n",
    "  \n",
    "# Create an instance of the Modis class\n",
    "modis = Modis(\"modis_2021_India.csv\")\n",
    "\n",
    "# Preprocess the data\n",
    "modis.preprocess_data()\n",
    "\n",
    "# Split the data into train and test sets\n",
    "X_train, X_test, y_train, y_test = modis.train_test_split()\n",
    "\n",
    "# Train the models\n",
    "stacking_regressor = modis.model_training(X_train, y_train)\n",
    "\n",
    "# Make ensemble predictions\n",
    "ensemble_predictions =modis.ensemble_predictions(stacking_regressor, X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "modis.evaluation(y_test, ensemble_predictions)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80b74cc5-d8ed-49d6-ba03-7551ede3c150",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
